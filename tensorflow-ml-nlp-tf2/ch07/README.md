# ch07. 사전 학습 모델(GPT)
## 03. GPT

### 1) GPT 1
- GPT(Generative Pre-training)1
  - 큰 자연어 처리 데이터를 비지도 학습으로 사전 학습한 후, 학습된 가중치로 풀고자 하는 문제를 미세 조정하는 방법론의 모델


- 모델 구조
  - 트랜스포머 모델 사용(버트와 동일)
  - 단, GPT1에서는 트랜스포머의 디코더 구조만 사용(버트는 인코더 구조만 사용)


- 사전 학습
  - 버트와 달리 하나의 사전 학습 방식(전통적 언어 모델 방식) 사용
  - 앞 단어를 활용해 다음 단어를 예측하는 방식으로 사전 학습 진행
  - 별도 라벨이 존재하지 않는 데이터도 학습 가능
    - 비지도 학습으로 분류
  - 많은 데이터로 모델 가중치를 사전 학습할 수 있음
  - 버트와 달리, 실제 문제 대상으로 학습 진행 시에도 언어 모델을 함께 학습
  
input|label
---|---
"< START>"|"나는"
"< START>","나는"|"학교에"
"< START>","나는","학교에"|"간다

### 2) GPT 2
- GPT(Generative Pre-training)2
  - OpenAI 제안 , 2018년 발표된 GPT1 모델의 성능을 향상한 모델로서 텍스트 생성에서 특히 좋은 성능을 보임
  
  
- 모델 구조
  - 대부분 GPT1과 동일, 트랜스포머의 디코더를 기반으로 하는 모델
  - 차이점: 레이어 노멀라이제이션이 각 부분 블록의 입력 쪽으로 이동 (기존에는 각 레이어 직후 레지듀얼 커넥션과 함께 적용)


- 학습 데이터 및 모델 크기

| |GPT1|GPT2|
|---|---|---|
|레이어|12개|117만 개|
|가중치|48개|1,542만 개|


- 모델 입력
  - BPE(Byte Pair Encoding) 방식을 사용해 텍스트를 나누어 입력값으로 사용
  - 글자와 문자 사이 적절한 단위를 찾아 나누는 방식으로 높은 성능을 보임
  
